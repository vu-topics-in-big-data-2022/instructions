{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"emr-spark-streaming.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPn0hpN/RdJacm70U2cVYq+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"source":["<a href=\"https://colab.research.google.com/github/vu-bigdata-2021/homework-6/blob/master/emr-spark-wordcount-example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"qEqUD2V9Pp9I"},"source":["!pip3 install boto3 pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z41093gblYuu"},"source":["from google.colab import files\n","\n","# upload AWS credentials\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95juzHk9bNCf","executionInfo":{"status":"ok","timestamp":1609904203312,"user_tz":360,"elapsed":488,"user":{"displayName":"Zhuangwei Kang","photoUrl":"","userId":"12885335612897739946"}}},"source":["import boto3, json\n","\n","with open('credentials.json') as f:\n","  credentials = json.load(f)\n","\n","session = boto3.session.Session(**credentials)\n","s3 = session.client('s3')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xu020yOtkReN","executionInfo":{"status":"ok","timestamp":1609904210306,"user_tz":360,"elapsed":470,"user":{"displayName":"Zhuangwei Kang","photoUrl":"","userId":"12885335612897739946"}}},"source":["# upload dataset to S3\n","s3.upload_file(Filename='sample_data/README.md', Bucket='vandy-bigdata', Key='datasets/README.md')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2VHMhTWjgd7"},"source":["%%file PysparkStreaming.py\n","from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext\n","\n","sc = SparkContext(appName=\"PysparkStreaming\")\n","words = sc.textFile('s3://vandy-bigdata/datasets/README.md').flatMap(lambda line: line.split(' '))\n","counts = words.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n","counts.saveAsTextFile(\"s3://vandy-bigdata/hw6/outputs/wordcount/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ednwRLerRjf","executionInfo":{"status":"ok","timestamp":1609904221904,"user_tz":360,"elapsed":355,"user":{"displayName":"Zhuangwei Kang","photoUrl":"","userId":"12885335612897739946"}}},"source":["# upload script to S3\n","s3.upload_file(Filename='PysparkStreaming.py', Bucket='vandy-bigdata', Key='hw6/PysparkStreaming.py')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgHeynlCXFJN"},"source":["# submit the new spark streaming job\n","CLUSTER_ID = 'j-xxxxxxx'\n","\n","emr = session.client('emr')\n","emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n","    'Name': 'Spark Streaming',\n","    'ActionOnFailure': 'CANCEL_AND_WAIT',\n","    'HadoopJarStep': {\n","        'Args': ['spark-submit',\n","                 '--master', 'yarn',\n","                 '--deploy-mode', 'cluster',\n","                 's3://vandy-bigdata/hw6/PysparkStreaming.py'],\n","        'Jar': 'command-runner.jar'\n","    }}])"],"execution_count":null,"outputs":[]}]}